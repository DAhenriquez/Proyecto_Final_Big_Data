# ğŸš€ Proyecto Final Big Data: E-Commerce Analytics Pipeline

**Asignatura:** Big Data  
**Estudiante:** [Tu Nombre Completo]  
**OpciÃ³n del Proyecto:** B (Pipeline AnalÃ­tico Batch con Spark + DuckDB)

---

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto implementa una arquitectura de datos moderna para analizar el comportamiento de usuarios en un e-commerce multicategorÃ­a. El objetivo es procesar millones de registros de interacciones (vistas, carritos, compras) para extraer valor de negocio mediante herramientas de Big Data.

El flujo de trabajo transforma datos crudos (CSV comprimidos) en un **Data Lake particionado** optimizado para consultas analÃ­ticas de alta velocidad.

## ğŸ—ï¸ Arquitectura de la SoluciÃ³n

El pipeline sigue un diseÃ±o por capas (Medallion Architecture simplificada):

1.  **Capa de Ingesta (Bronze):** Lectura masiva de archivos `.csv.gz` utilizando **Apache Spark**.
2.  **Capa de Procesamiento (Silver):** Limpieza, tipado y particionamiento de datos por fecha (`event_date`) almacenados en formato **Parquet**.
3.  **Capa de AnÃ¡lisis (Gold):** Consultas OLAP de alto rendimiento utilizando **DuckDB** (lectura *Zero-Copy*).
4.  **Capa de VisualizaciÃ³n:** Dashboard interactivo integrado en Jupyter Notebook usando `ipywidgets` y `matplotlib`.

## ğŸ“¦ Estructura del Repositorio

```text
/
â”œâ”€â”€ notebook_proyecto_final.ipynb   # Notebook principal con todo el cÃ³digo
â”œâ”€â”€ requirements.txt                # LibrerÃ­as necesarias
â”œâ”€â”€ README.md                       # Este archivo
â”œâ”€â”€ /csv_data.zip                   # âš ï¸ DATOS CRUDOS (DESCOMPRIMIR)
â”œâ”€â”€ /silver_lake/                   # (Generado por Spark) Data Lake particionado
â””â”€â”€ /assets/                        # ImÃ¡genes o diagramas (opcional)
